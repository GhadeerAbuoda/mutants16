\section{Related Work}
\label{sec:related}

%There are four primary areas of related work.  First, fuzzer taming and related problems of determining the set of faults from a large set of redundant failures have been investigated in a few papers.  Second, some recent efforts to localize faults have begun to incorporate the use of program mutants, and the broader problem of fault localization has been very extensively investigated.  Program mutants have also been used in efforts to produce automatic repairs for program faults.  Finally, our work fits into the general framework of efforts to define distances between program executions.
 
The most closely related work is that of Chen et. al \cite{PLDI13}.  We use two of the same compiler data sets, and both apply the FPF \cite{Gonzalez} algorithm.  The distance metrics used with FPF in the previous work, however, were \emph{ad hoc}, chosen for each program by hand, produced poorer results for the critical portion of the curve for the two hard data sets, and were not extended to provide any fault localization.  This paper uses the FPF approach in the context of a more principled causal comparison of program executions that applies in theory to any executions, not just failing test cases, and introduces novel fault localizations.  Prior to the FPF-based work, Francis et al. \cite{Podgurski04}, Podgurski et al. \cite{Podgurski03} and others \cite{Liu06,Liblit05} used clustering to attempt to solve similar problems in identifying distinct faults, on simpler human-generated data sets.  Chen et. al \cite{PLDI13} provide an in-depth comparison of clustering and ranking, and the advantages of ranking (as well as some possible limitations).  Jones et al. discuss general issues in debugging multiple faults efficiently \cite{Jones07}.

Fault localization \cite{FaultSurvey,NearNeighbor,Liblit03,Liblit05,Cleve05,Jones2002,Jones05,Jones07,GroceError,ChakiLev,Liu06,SPIN03,Santelices:ICSE:2009,Abreu:2006:PRDC,xuan2014test,DD}, even specifically for compilers \cite{Whalley94}, is a long-standing field of research.  Recently, fault localization researchers have used program mutants to improve statistical \cite{Jones2002} fault localization techniques \cite{MUSE,DebroyMutant,FasterMutant,multilingual,Metallaxis}.
%The core of this work has been the belief that faulty program locations are (1) more likely to change a failing test to successful when mutated and (2) less likely to change a successful test case to faulty when mutated.  This clearly connects to our general notion that how two test cases respond to a set of mutants is a good measure of their semantic similarity.
The work of Hong et al. \cite{multilingual} on MUSEUM and Moon et al. \cite{MUSE} on MUSE provides a good summary of other work along these lines, and is perhaps most similar to ours in assumptions, in particular ``Conjecture 1'' that a test case that used to fail on a program is more likely to pass on a mutation of a faulty statement.  While we suggest some localizations as a welcome and interesting side effect of our distance metric and FPF analysis, our primary goal is the discovery of all faults. Much fault localization work is largely (in many cases exclusively) based on single-fault scenarios; e.g., the MUSE \cite{MUSE} evaluation includes only 2 of 14 scenarios with multiple faults, and those scenarios include only a few faults.   The statistical approaches may owe some of their superiority in experiments over other methods to the use of mostly single-fault evaluations \cite{Jones05}.  By focusing on a single failing test case (for us, selected by fuzzer taming) as the locus of analysis, our work, like some early efforts \cite{NearNeighbor,Cleve05,GroceError} may be more suited for situations involving many faults.  MUSEUM \cite{multilingual} also revisits this approach, combining the MUSE formula with localization using only a single failure. 

Using program modifications or mutants broadly defined to repair programs has been a popular topic of recent work as well \cite{GenProg,AutoRep,WeiFix}.  These \emph{generate-and-validate} approaches to patching aim to fix programs by generating a large variety of potential fixes, then using test cases to prune out invalid fixes.  Qi et al. \cite{achour} discuss some serious limitations of early work on this topic, but also demonstrate that effective patching is possible using a restricted search space that focuses on removing functionality.

Some approaches to localization (and all approaches to patching) assume the possibility of a mutant \emph{actually} fixing a fault.  In our experiments, we found this to be highly unlikely -- most of our patches were far larger than the largest higher-order-mutants \cite{jia2009higher} anyone usually considers applying, and none were equivalent to a repair.  Previous work has shown that, across a large body of open source programs in many languages, most fault fixes were larger in size than even low-order higher-order mutants \cite{GopinathMutants}.
%We speculate that fixes for compilers may be likely to be even more complex than the average, given that compilers are particularly complex software systems.  
%Our primary assumption is simply that similar executions respond similarly to mutants, and in particular executions due to the same fault can be ``repaired'' by similar program changes.  These changes are likely \emph{related} to the actual fault, but are unlikely to be the actual fault, for reasons of comparative complexity of mutants and real fixes.

The use of distance metrics in software engineering includes some of the localization and fault identification efforts discussed above (e.g., \cite{NearNeighbor,GroceError,ChakiLev,Liu06}).  Vangala et al. proposed using distance to cluster test cases to improve diversity and eliminate duplicates \cite{VangalaDist}.  Adaptive random testing uses distances (usually between inputs) to choose tests \cite{Chen,ARTChen,ISSTAART}, and Artzi et al. guided test generation for fault localization using path constraint metrics \cite{ArtziDirected}.  Sumner et al. evaluated approaches to generating execution peers for debugging and understanding \cite{Sumner2011} using tree edit distances (and evaluated other distances).  Xin et al. \cite{Xin2008} examined methods for indexing program executions, useful in all methods involving execution alignments.  To our knowledge, most metrics used are essentially spectrum-based \cite{RepsSpectra,BallConcept} (using counts over structural entities), while our metric is essentially causal, based on counterfactual \cite{LewisCause,LewisCount} programs.