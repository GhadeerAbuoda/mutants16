We thank the reviewers for very helpful comments.

R1:

Q: ...do you observe a correlation between distinct test cases and distinct invariants?

Even reduced test cases are hard to connect to invariants, tend to execute 10K+ LOC (see https://github.com/agroce/mutants16/tree/master/tests).  There probably is a correlation, but it's hard to spot for compiler faults.  We'd definitely like to investigate further.

Q: ...all bugs... due to... optimizations. Is this correct? ...do these... the majority of bugs...? ...results to change for non-compiler benchmarks?

For GCC ~90% of faults detected by changing from -O0 to -O3, so seems to be the rule for GCC.  For SpiderMonkey, "optimization" is hard to separate from odd JavaScript semantics, or attempts to speed up SpiderMonkey in general (not neccessarily code optimization exactly).  Results of Qi et al. suggest many programs _can_ be "patched" by turning off a feature, which is promising.

3) Section 1.2: “if two failures.., baseline probability*. What does this mean?

(Baseline) B = chance two failing tests are due to same fault.  Knowing two failures are "fixed" by turning off same optimization indicates probability they are same fault is > B.  We quantify (for our subjects) how much this holds (though not for optimization changes alone).

R2:

Q: ...fault of the center... repairs many others? Will that fault be discovered?

Really depends on the overall structure (what repairs those other tests).

Q: ...faults not discovered in the experiment...?

No obvious patterns, but differences would be complex properties of C/JavaScript and GCC/SpiderMonkey.  We'll investigate further.

Q: ...sampling does alter the results... undesirable happening in terms of the onion ring model.

Sampling hurts localization, there are probably ways to do good sampling for fuzzer taming (we are investigating).  Our guess: localization is bad when nothing in the middle of the onion ring is left, and that set could be small.

We'll provide numbers on runtimes (JavaScript not bad, GCC pretty bad, but we ran build/mutants in a very inefficient way).

R3:

Q: ...what kind of changes... ...does that count?  ...insert System.exit(0) at the beginning...?

Anything such that the test oracle passes is allowed.  For a very weak oracle, you could get away with this.  Due to onion ring, letting some "silly" changes make tests pass is probably harmless, so long as other more interesting mutants are also used.

Q: ...don’t quite understand... evaluation results...

Figure 2 shows _only_ fuzzer taming results, Tables 1-2 are _only_ for localization results.  These are two different goals, and evaluated separately.  FPF value only in Tables to show that how well a fault localizes and how well it is identified seem (surprisingly) unrelated.

Q: ...role of passing tests is "lower" than that of failing tests?

We believe so: probably for fuzzer taming, possibly for localization (note MUSEUM and other localizations agree).  Faulty code often executes in many passing tests; determining conditions for failure is difficult.  There is more information _about faults_ in failures, in some sense: e.g., single failing test can show a fault exists, which all passing tests cannot!

Q: ...how to interpret... Figure 3...

992 (commented "ADDED") is part of the code in the patch, as is the deleted statement (line #s in patch would help).  We'll clarify this much better, sorry!  3/6 is indeed ranked change.

Q:  ...confirm that these 8 faults are correct...?

For these 8 (and 16 GCC faults), we had a backpatch, based on commit data and comments, and tested it.  The other faults had such complex change history this was not feasible, making localization (not taming) evaluation impossible.  Backpatch sets were produced by colleagues, not us, before we began this work (so are not cherry-picked).
