\section{Conclusions and Future Work}
\label{conc}
When are two failing test cases due to the same fault?  This paper
demonstrates that
``when they are repaired by the same program mutants'' is a useful answer to
this question.  The problem of causality in executions is hard, and
has long been tied to a notion of similarity of
executions \cite{NearNeighbor,GroceError}.  By using program changes
(produced by mutants) as
causes to determine distance, it is possible to improve on efforts
to identify the set of faults in a large set of redundant compiler failures
\cite{PLDI13} and discover the source code locations of faults
\cite{MUSE,multilingual}.  For a set of more than 2,800 tests and more
than 50 faults, over the Mozilla SpiderMonkey 1.6 JavaScript compiler and
GCC 4.3.0, our methods improve fault identification (``fuzzer taming''), in a statistically significant way
(with effect size of about 3\% improvement), over
the \emph{best} (for each subject program) of over 16 metrics
considered in previous work.

We also demonstrated that our approach can be cheap and effective in
fuzzer taming not targeting compilers.  It performed much better than
a coverage-based ranking approach, reliably revealing distinct faults
in the first few inputs presented, and is more general in application
than the (to our knowledge) best competing method, semantic crash
bucketing \cite{SCB}.

Preliminary investigation also shows improvement (for a shared
mutation testing budget) over previous mutant-based fault localization
methods, for the same tests and a subset of 24 faults with known
locations.  Our {\bf Repair} method, in particular, was the only
localization approach to provide any perfect localizations of faults
(it produced 3),
and provided three times as many very-high-quality (ranking the fault
in top 5 locations) localizations as the next best method
\cite{multilingual}  --- 6 such localizations, compared to only 2.
For GCC wrong-code faults, only our methods produced any
very-high-quality fault localizations.  When applied to non-compiler
faults, the metric tended to either not produce a localization,
produce a very small incorrect localization (1-3 statements), or
produce a very high quality localization.  Restricting to the cases
where we consider {\bf Repair} applicable (7 faults) {\bf Repair}
perfectly localized 4 of them, and produced a 1 or 2 statement
incorrect localization for the others.  

As future work, we plan to apply the mutation-based metric to other
fuzzer taming and fault localization problems.  The highly varying
results for even the best fault localization methods in our
experiments demonstrate that further advances are required before
compiler debugging can consistently be made less onerous through automated
assistance.

More practically, we would like to tightly integrate our approach with
automated test generation tools, such as TSTL \cite{tstlsttt,NFM15,ISSTA15},
DeepState \cite{DeepState,DeepStateTutorial}, Echidna \cite{Echidna}, and
Manticore \cite{Manticore}.  Automated test generation tools tend to produce a
large number of passing and failing tests, and are often used in
``overnight'' runs where adding a step to use mutants to triage
failing tests and provide localization suggestions for the highly
ranked tests may not even impose a noticable overhead on current usage
practices.  The latter two tools (Echidna and Manticore), which target smart
contracts for the Ethereum blockchain \cite{wood2014ethereum}, are
particularly promising opportunities, because the gas-limited computation bounds of the Ethereum
Virtual Machine essentially guarantee that smart contract code is
small enough to make mutation analysis cheap.  The availability of a
multi-language mutation analysis tool \cite{RegExpMut} supporting
Python, C/C++, and Solidity (the language used for most smart
contracts) means that a common infrastructure base can be shared by
all of these tool integrations.

We also plan to explore other applications of mutant-based metrics.
Many software engineering techniques rely on measuring distance
between program executions \cite{BallConcept,NearNeighbor}.  Not all
of these concern only failing
executions.  For example, Zhang et al. \cite{issta14} show that FPF based on a simple
Hamming distance over branches covered can significantly improve the
effectiveness of seeded symbolic execution
\cite{Zesti,PersonSeed,BugRedux}.  Mutation response is likely too
expensive to use for this purpose without modification, but sampling
mutants to refine distance when coarser metrics plateau
may be practical.  Cost may be less of a concern when applying our
metrics as a method for prioritizing or selecting regression tests,
where the results can be repeatedly used in test suite execution,
without recomputing distances based on every code change
\cite{YooHarman}.  Our metrics may also be useful in determining
execution peers \cite{Sumner2011}, for example to help operators of
spacecraft find similar behaviors to telemetry downlinks in testbed
history \cite{KlausRajeev,scriptstospecs}.

{\scriptsize {\bf Acknowledgements:} The authors would like to thank the authors of
the MUSEUM \cite{multilingual} and Metallaxis-FL vs. MUSE 
\cite{Papadakis} studies for access to their data sets.  In
particular, we thank Shin Hong and Thierry Chekam for prompt
assistance beyond the call of duty.  We would like to thank the
authors of the Semantic Crash Bucketing \cite{SCB} study, in
particular Rijnard van Tonder and Claire Le Goues, for their assistance in
comparing our results with SCB.}