\section{Experimental Results}

\begin{figure}
    \centering
        \includegraphics[width=1.0\textwidth]{jscurve}
        \caption{Discovery curves for SpiderMonkey 1.6 faults}
        \label{jscurves}
 \end{figure}%
    ~ 
    \begin{figure}
        \centering
        \includegraphics[width=1.0\textwidth]{gcccurve}
        \caption{Discovery curves for GCC 4.3.0 wrong-code faults}
        \label{gcccurves}
    %\caption{Discovery curves compared to best curves from Chen et. al PLDI 2013 data (FPF benchmark)}
\end{figure}


%\begin{figure*}
%\centering
%\includegraphics[width=2\columnwidth]{jscurve}
%\caption{Discovery curves for SpiderMonkey 1.6 faults}
%\label{jscurves}
%\end{figure*}


%\begin{figure*}
%\centering
%\includegraphics[width=2\columnwidth]{gcccurve}
%\caption{Discovery curves for GCC 4.3.0 wrong-code faults}
%\label{gcccurves}
%\end{figure*}

%\subsection{General Experimental Framework}

Our primary experiments are based on subjects used in the only previous study of compiler fuzzer taming (from Chen et al.'s PLDI 2013 paper \cite{PLDI13}), which we hereafter refer to as the FPF benchmark.  Of the three data sets examined in that paper, this paper considers two:  faults and tests cases for SpiderMonkey 1.6, Mozilla's JavaScript engine, with tests generated by {\tt jsfunfuzz} \cite{jsfunfuzz}, and \emph{wrong code} faults and test cases for GCC 4.3.0, with tests generated by Csmith \cite{csmith}.  GCC 4.3.0 crash faults were essentially perfectly localized by previous approaches.  In fact, on examination, only two of the 11 crash bugs are not distinguished by simply examining the crash message.  Faults that produce a crash are also likely to be much easier to debug than semantic problems such as all of the GCC wrong code bugs and most of the JavaScript faults --- e.g., following a dynamic slice might well suffice.

Our mutants were produced using the tool written by Jamie Andrews \cite{mutant}.  Andrews' tool applies only four operators: statement deletion, conditional negation, operator replacement, and constant replacement, chosen as a small set that still produces good results for C code.  Experiments were performed on a MacBook Pro with 16GB RAM and dual-core 3.1GHz Intel Core i7; GCC executed on a VirtualBox-hosted Ubuntu 11.04.  For GCC, some test cases from the FPF set no longer failed, presumably due to unknown differences in execution environment, OS, or memory layout. Discarding these reduced the set of test cases from 1,275 to 1,117 and the number of distinct faults to 27 rather than 35.  For SpiderMonkey, all 1,749 test cases from the FPF benchmark failed in the new environment, representing 28 distinct faults.

These data sets, though similar in that both compilers are written in C, provide some interesting variance for testing our metrics.  The oracle for SpiderMonkey executions is the set of checks built in to {\tt jsfunfuzz} plus the requirement that the execution not crash.  This is only a moderately strong oracle, and allows serious deviations from both the JavaScript language specification and normal SpiderMonkey behavior, while checking some complex details, such as {\tt eval} round-trips.  For GCC, the oracle is a differential check on a hash code:  failure was defined as producing a executable with the -O3 flag that, when executed, produced a different checksum than code compiled by either of GCC 4.9.3 or clang 7.0.0, both using -O0.  A repairing mutant must enable GCC 4.3.0 to actually compile code correctly, which is a very strict correctness property, making coincidental correctness \cite{CCT} highly unlikely --- only missed optimizations are allowed.

We only show results for the first 50 tests in the ranking, and computed areas under curves for the same limit, since it seems very unlikely that a user will examine many more than 50 tests, especially given the decreasing slope of the discovery curves.  In practice, after 50 tests, fixing a few faults and then re-running tests and FPF seems the most likely recourse, and we confirmed that after removing random subsets of faults, our metrics still outperform the previous best.  We applied X-means \cite{xmeans} in an attempt to use clustering with our metrics, but, as with previous results \cite{PLDI13} it did not compare well with FPF, and the runtime was much higher.  We confirm the conclusion \cite{PLDI13} that clustering (at least with X-means) does not work well in a setting with extreme disparity in instance counts for faults.  A further difficulty for our setting is that X-means and most off-the-shelf clustering tools take vectors, not a distance metric.  X-means is using the raw repair data, not weighted by the fact that 0-0 agreement is much less informative than other possible matchings (a Euclidean over 0-1 vectors is just the square root of Hamming distance).

\subsection{Mozilla SpiderMonkey 1.6 Results}


There were 96,828 SpiderMonkey mutants, based on 69,634 lines of code in C and header files.  Of these mutants, 12,666 were covered by some failing test case.  Of these mutants, 10,525 survived a basic set of SpiderMonkey quick tests \cite{icst2014}.  Of these mutants, 1,326 (12.6\%) repaired at least one test case.  Figure \ref{jscurves} shows the discovery curves for the mutant repair metric compared to the ideal discovery curve and the best curve from previous work using FPF, which used a normalized Levenshtein distance \cite{lev} over the failure output and test case text \cite{PLDI13}.  A discovery curve is a plot of the number of distinct faults that a user, examining the tests in the ranked order, would have seen after N tests (here, N goes up to 50).

The APFD (Average Percent Faults Detected) values in the graphs are based on the measure  introduced by Rothermel et al. for evaluating test case prioritization methods \cite{APFD}.  APFD is a somewhat better summary of results than the raw curve areas used for evaluation in previous work \cite{PLDI13}.  APFD, as the name suggests, measures the percent of all faults discovered at the ``average'' point on the curve, by comparing the curve's area to an ideal curve, with interpolation.  A simpler (but less informative) way to compare the curves is to note that the mutation-based metric's curve is above the best previous curve at 60\% of data points\footnote{The details of APFD calculation are slightly involved, and the interpolation and perfect curve are not exact fits for fuzzer taming; nonetheless the basic method summarizes curves fairly well and is standard in the testing literature for similar problems \cite{issta14}.}. 

APFD results are useful summaries, but the curve itself is also worth examining, since (1) long sequences of tests with no new faults may discourage users more than an overall less effective but steadily climbing curve with few ``plateaus'' as we call these uninformative sequences of tests and (2) a good early curve is important to developers.  The mutant repair curve climbs very rapidly in the early portion, with perfect discovery for the first 12 faults. The largest plateau is 5 tests without a new fault, ignoring the long period after the 31st test.  In fact, a plateau at the end of the curve is not problematic.  Assuming that few users will examine more than 50 test cases without fixing some faults, the user may give up after seeing 10 or more tests without a new fault based on the mutant repair curve, and in fact lose nothing by doing so.  The best FPF benchmark curve, in contrast, has a plateau of size 17 after the 17th fault.  If we assume a user stops examining tests after a size 10 or greater plateau, the user will only see 17 faults using the best benchmark metric, vs. 19 with our metric.  Stopping at a plateau of size 6 produces the same results.

Over all mutants, and all pairs of test cases repaired by the same mutant (so the same pair may count many times, if many mutants repair both test cases), the probability of being due to the same fault was 42.77\%, and the probability of being due to the same fault if two test cases disagreed on a repair (the mutant repaired one test case but not the other) was only 20.33\%.  The baseline rate for same-fault for test pairs was 33.64\%.  Just knowing that two test cases have \emph{one} shared repairing mutant makes it 1.27x more likely that they are the same fault, and knowing they differ for one mutant makes it 0.6 times as likely they are due to the same fault.  Matching non-repair, however, provides very weak evidence:  only a 34.14\% chance of matching fault, just over baseline.

An additional measure of effectiveness is to consider how effective a metric is in producing matched nearest neighbors.  That is, how often is the nearest neighbor of a failure (that is not due to a singleton fault --- a fault detected by only one test case) due to the same fault?  For reduced \cite{DD,PLDI13,
CReduce} test cases, we assume that for a perfect metric, the nearest neighbor should almost always share the same fault, since there is little or no extraneous semantic content to each test beyond the cause of failure.  For the SpiderMonkey failures, 96.3\% of non-singleton failures matched their nearest neighbor(s).  For mutants that repaired any failures, the mean and median number of test cases repaired were 120.6 and 4.0, respectively.  Most mutants repaired a small number of tests, but a few mutants repaired a very large number of tests.  A few mutants repaired \emph{all} SpiderMonkey faults; obviously these mutants were not actual fault fixes, but effectively disabled the mechanism {\tt jsfunfuzz} used to detect failure. The mean and median homogeneity for repairing mutants (\% of failures repaired corresponding to the most common fault repaired) were 77.96\% and 86.36\%, respectively.  

\begin{figure}
  \centering
  \includegraphics[width=0.95\columnwidth]{comparejs}
  \caption{APFD values for Spidermonkey suite slices}
  \label{comparejs}
\end{figure}%


In order to check our results statistically, we sliced the FPF benchmark tests into 20 randomly selected equal-sized (as much as possible) distinct subsets (Figure \ref{comparejs}) .  The repair metric had a mean APFD (89.23\%) for these subsets that was 2.85\% better than the mean APFD for the best benchmark metric (86.76\%), over sets of $\sim 10$ faults. The result was statistically significant ($p < 0.05$ by Wilcoxon test).   Median repair APFD was 90.05\%, vs. 84.75\% for the best benchmark metric.


\subsubsection{Fault Localization}

Table \ref{bothtable} shows a comparison of three fault localization methods for the SpiderMonkey (and GCC) faults.  The first column is a fault ID. The next three columns show localization rankings.  A ranking of 1 means that a code location \emph{contained in} the actual faulty code locations was presented to the user as the most likely location of the fault by the method: the method has precisely identified the fault location.  A ranking of, for instance, 22, on the other hand would mean that a user examining code locations proposed as faulty by the method would not reach any faulty code until examining the 22nd such location proposed by it.  A dash for a column means that localization did not rank any faulty statements, or assigned all faulty statements suspiciousness of 0.0.  The three rankings are: our {\bf Repair} localization, the MUSE \cite{MUSE} localization, and the MUSEUM \cite{multilingual} localization.  MUSEUM uses the same formula as MUSE, but works better for multiple faults because it uses only one failure.  The MUSE/MUSEUM formulas normally make use of information from passing tests as well: when mutating a statement makes a passing test case fail, it makes the statement \emph{less} suspicious, by a weighted amount. The weight assigned to information from passing tests in our setting would likely be low (due to the ratios of repairs to mutation kills).  In a limited sense, information from passing tests is already incorporated in our results.  Throwing out all mutants that are killed by any passing test as potential repairs/localizations ensures that the rankings of all statements that are in MUSE/MUSEUM rankings are correct, \emph{relative to each other}.  By definition, passing tests have no influence on the suspiciousness of these mutants.  However, there may be other mutants that 1) repair a failure and 2) are killed by some test case: these could, if enough different ones repaired the same faulty statement, improve MUSE/MUSEUM results, though in practice this is extremely unlikely.  We reject such mutants in part to keep costs low, and in part because we think that the causal information contained in such mutants, that ``fix'' a failure but also break some passing test(s), is problematic.  They seem likely to be less useful as explanations of the causes of a failure, since they do not impact the program semantics in a way that is only known to be beneficial, and could potentially even mislead a user if used as explanations.

Discussion with the MUSE/MUSEUM authors confirms  that adding information for passing tests, while costly, would likely improve the MUSE and MUSEUM results, given the weakness of our oracles, particularly for SpiderMonkey, and should be considered essential for ideal application of their approach.  Because the cost of recording the full mutant analysis matrix for all passing tests (rather than considering a mutant killed as soon as one test fails, as we do) is very high, and we would like to produce a comparison over a fixed computational cost, we give results for MUSE/MUSEUM over failing tests only, cautioning that we are not sure what the impact of this choice is on faults that were not localized.  For similar reasons of keeping costs low, we also used the mutation operators of Andrews vs. the more extensive Proteum \cite{Proteum} operators used in MUSE/MUSEUM's evaluations.  Even with these restrictions, running all mutants on passing tests until at least a single kill was observed took about 3 times as long as the search for repairs, which we already consider a potentially problematic cost.  In Section \ref{sec:otherfaults} we provide some comparisons with MUSEUM and MUSE fault localization in the context of a full mutation result matrix using their preferred set of operators.

Results are reported as absolute ranks, rather than more sophisticated \cite{MUSE} localization evaluations because our localizations do not assign suspiciousness scores, only rankings, and in accord with the proposal of Parnin and Orso that only localizations ranking a fault in the top few statements may be useful to users \cite{AutoHelp}.  We expect users to examine the mutants and consider how they help avoid the fault.  Blind, unaided pointers to portions of compiler code without more semantic information about why the statement is suspicious seem unlikely to work well, based on our experience with complex compiler faults, and the results of Parnin and Orso \cite{AutoHelp}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{bothtable} only includes 8 of the 28 SpiderMonkey faults.  This is due to a problem with the data set:  producing ground truth patches for a large application, in the sense of finding a minimal, clearly fault-fixing code change that can be back-ported to the original code is difficult \cite{PLDI13}.  While we believe the 28 faults identified in the FPF benchmark data are correct, it is very difficult to produce a valid patch of version 1.6 that captures the fix for most of these faults.  In some cases the final commit that caused tests to stop failing appeared to only be the end of a complex series of changes that converged on a correct fix.  In other cases, the code was modified so extensively before the fix that identifying ``the incorrect part'' of the original code seemed to owe more to guesswork than certainty.  The evaluation of localization therefore only examines the 8 faults for which we could be reasonably certain that the patch to 1.6 was correct and characterized the fault in question accurately.  In no case was the patch equivalent to one of the mutants (the smallest patch modified two statements).

No method performs extremely well --- for half the faults, no methods produced what we consider a useful localization.   Compiler faults are hard to debug, and any help is useful, but it seems unlikely developers will really benefit from a localization when it does not rank at least one faulty statement in the first 25-30 statements.  By this measure, MUSE only provides a helpful localization once, and performs worst of the methods.  {\bf Repair} and MUSEUM both perform well, with {\bf Repair} slightly better.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{comment}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        %
% \subsubsection{Error Explanation}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      %
%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        %
%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        %
                                                                                                                                                                                                                                                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{GCC 4.3.0 Wrong Code Results}


\begin{figure}
  \centering
  \includegraphics[width=0.95\columnwidth]{comparegcc}
  \caption{APFD values for GCC suite slices}
  \label{comparegcc}
\end{figure}%

There were 377,679 GCC mutants, based on 424,186 lines of code in C and header files. Of these mutants, 73,016 were covered by some failing test case.  Of these mutants, 41,385 survived a GCC bootstrap build, compiled hello world, and passed a GCC test suite.  Of these mutants, 3,232 (7.8\%) repaired at least one test case.  Figure \ref{gcccurves} shows discovery curves for the mutant repair metric vs. the best benchmark curve, Euclidean distance over function coverage vectors \cite{PLDI13}.  GCC's APFD improvement is larger than that for SpiderMonkey, but its early curve (first 15 tests) is worse compared to the benchmark curve, but still has a maximum gap of only 2 redundant tests vs. 4 for the benchmark.  Over all, again the curve is better than the best benchmark curve at 60\% of all points.  Using the hypothetical model where a user stops examining tests after a plateau of size 10, the user of the mutant-based taming will see 18 distinct faults by examining 25 tests, and the user of the function coverage will see 20 faults after 45 tests.  If a user gives up after a plateau of size 5, our approach again lets a user examine 18 faults over 25 tests.  Function coverage only yields 16 faults over 31 tests. 


For GCC wrong code faults, the baseline chance that two test cases shared an underlying fault was 38.97\%.  Knowing that two test cases shared a repairing mutant raised the chance to 68.5\%, and knowing they disagreed on a repair lowered it to only 19.01\%.  The respective increase and decrease in probability of matching fault compared to baseline was thus 1.75 times greater for matching repairs, and less than 0.5 the chance of being the same fault, given one mismatched repair.  Knowing two test cases were both not fixed by a mutant again provided marginal evidence of sharing a fault: 39.2\% chance of matching faults.
For 99.3\% (all but 8) of the 1,090 non-singleton fault failures in GCC wrong code, the closest test case(s) by the repair metric showed the same fault.  The best previous reported FPF metric, function coverage vectors, had a matching rate of 92.2\%.   The mean and median numbers of repaired tests per mutant (for mutants that repaired any tests at all) were 18.4 and 3.0, respectively.  A few mutants fixed a very large number of tests, up to 1,050.  These all appear to be turning off all optimizations --- essentially running gcc in -O0 mode.  Most mutants fixed only a few tests, to a greater extent than was true with SpiderMonkey.    Mean homogeneity was 79.4\% (median 100\%).

Slicing the GCC tests into 20 random equal-sized test subsets and comparing with function and line coverage metrics (Figure \ref{comparegcc}) we find that differences in APFD values are statistically significant between our metric and both benchmark metrics, by Wilcoxon test, with $p < 0.0005$, and the mean APFD improvement --- even for only 55 tests, exposing only 6-11 faults (mean of 7.75 faults) --- is more than 3.8\% better than either benchmark metric.


\subsubsection{Fault Localization}

\begin{table}
\caption{Fault localization for compiler faults}
\centering
{\scriptsize
\begin{tabular}{|c||c|c|c||c||c|c|c|}
\hline
%& & \multicolumn{4}{|c|}{Localization Rank} \\
%\hline
\multicolumn{4}{|c||}{SpiderMonkey}&\multicolumn{4}{|c|}{GCC}\\
\hline
ID& Repair & MUSE & MUSEUM & ID & Repair & MUSE & MUSEUM \\
\hline
1 & - & - & - &         1 & - & 157 & -\\
2 & - & 173 & - &     2 & 1 & 99 & 14\\
3 & 1 & 22 & 4 &       3 & - & 146 & -\\
4 & - & - & - &         4 &  - & - & -\\
5 & 19 & 118 & 26 & 5 & - & 109 & -\\
6 & 28 & 87 & 23 &   6 & - & - & -\\
7 & - & - & - &         7 & - & - & -\\
8 & 3 & 133 & 5 &     8 & 4 & 158 & 18\\
& & & & 9 & - & - & -\\
& & & & 10 & - & - & -\\
& & & & 11 & 10 & 167 & 6\\
& & & & 12 & 1 & 172 & 195\\
& & & & 13 & - & - & -\\
& & & & 14 & 34 & 170 & 125\\
& & & & 15 &  5 & 159 & 31\\
& & & & 16 & - & - & -\\
\hline
\end{tabular}
}
\label{bothtable}
\end{table}

Table \ref{bothtable} shows localization results for GCC 4.3.0.  Again, only some faults were deemed to have strong enough ground truth patches for evaluation.   MUSE performed poorly, with no useful (by the standard of having at least one faulty statement in the top 25) localizations.  MUSEUM provided 3 useful localizations, but no very high quality localizations.  Only {\bf Repair} performed very well, with useful localizations for 5 faults, and the fault was in the top 5 statements for 4 of these.

\subsection{Coverage Localization}
\label{sec:covloc}

While {\bf Repair} is often a high-quality localization technique, it often fails to localize a fault at all.  This failure suggested that for the compiler setting, we should consider other localizations methods, even ones we do not consider likely to be helpful in more general settings.  Compiler faults are often so complex and hard to debug that even a somewhat informative localization might be useful.

By definition, when a mutant repairs a test case, it changes the
behavior of the test.  In some cases, this behavioral change may be
limited to changes in program state and data values.  However, the
vast majority of mutant repairs \emph{for a compiler} can be expected to also modify the code coverage of a
test; in our compiler experiments, there were no repairs that did not change
coverage; this would be very unlikely to hold in other software settings, where many bugs related to data values; for well-fuzzed compilers, however, few bugs are simple, e.g., off-by-one bugs in an integer value.  If a fault can be avoided by disabling a
problematic optimization in a compiler, this naturally leads to
changed coverage.  If the fault is due to a bad conditional, where a
program path should include additional code, the repair will often
change the execution to include the omitted code.  As a localization
for a fault, then, we can examine the source statements whose
coverage changed most often in repairs.  Early experiments with this
 approach showed that a useful coverage change localization must  account for the frequency of a
change.  Some coverage changes are extremely common across all
failures, faults, and repairing mutants,
and therefore likely not related to any specific fault.

\begin{quote}
The {\bf Coverage} localization for a failing test case $f \in F$ ($F$
is the set of all failing test cases) ranks
statements by the function $r$ (for rank) where:
%$$U = s : \forall m . \forall t \in F . c(m,t,s) = 1$$
$$C(s) = \sum_{t \in F} (\sum_{m \in R(t)} c(m,t,s))$$
%\[
$$r(s,f) = \sum_{m \in R(f)}\frac{1}{C(s)}c(m,f,s)$$
%\begin{cases}
% \sum_{m \in R(f)}\frac{1}{C(s)}c(m,f,s) & s \not\in U\\
%   0 & s \in U
%\end{cases}
%\]
$c(m,t,s)$ is 1 if coverage of $s$ changes for test $t$
with mutant $m$, and 0 otherwise.  $R(f)$ is the set of mutants repairing $f$.
\end{quote}

That is, the ranking is based on the number of times a statement's
coverage changes its value (covered or not covered) from the original
execution of $f$ in executions of mutants that repair $f$.  Changes are weighted by their inverse frequency over all
failures and all mutants.
Higher values for $r$ result in a higher ranking (more likely to
be faulty).  

The coverage localization also provides a kind of explanation, though
a less direct one than with a repair.  For any
interesting line of code in the coverage-change-based localization,
the user can investigate a particular instance of the change,
by selecting a single mutant and stepping through the execution of the
failing test case in a debugger to the point of change.  The user can also
browse through all mutants that produce this coverage change for the test
case.

\begin{figure}                                                
{\scriptsize                                                  
\begin{code}                                                  
{\bf Diff of old ($<$) vs new ($>$) for SpiderMonkey fault 8 patch (portion):} 
...                                                           
<             vp[1] = OBJECT\_TO\_JSVAL(thisp);               
<         \} else \{                                          
<             ok = OBJ\_GET\_PROPERTY(cx, thisp, id, \&v);    
<         \}                                                  
...                                                           
<                   a->avail = (jsuword)sp;                   
<               \}                                            
...                                                           
---                                                           
>         RESTORE\_SP(fp);                                    
...                                                           
{\bf Mutant information:}                                     
Failure repaired by 148 mutants.                              
...                                                           
\#3: delete statement mutant of jsinterp.c:944:               
                                                              
            vp[1] = OBJECT\_TO\_JSVAL(thisp);                 
        \} else \{                                            
/*MUTANT    ok = OBJ\_GET\_PROPERTY(cx, thisp, id, \&v);*/    
        \}                                                    



{\bf Coverage change information:}
...
\#6: 5 mutants (3.38\% of repairs) added jsinterp.c:992:
                   a->avail = (jsuword)sp; /* ADDED */                                                            
\end{code}                                                    
}                                                             
\caption{Patch and explanation for fault 115}                 
%\vspace{-0.18in}                                             
\label{fig:explain}                                           
\end{figure}                                               


Figure \ref{fig:explain} shows part of the patch for SpiderMonkey fault 115, and the {\bf Repair} and {\bf Coverage} outputs that successfully localize the fault (the 3rd mutant output, and the 6th coverage change output), to give some idea of the information our methods present.  For {\bf Coverage}, it is surprising that the sixth highest ranked change only appeared in 5 of 148 repairs. This change ranked high out of thousands of changes because it was unusual, appearing for only 10 repairs in the entire SpiderMonkey analysis. 

\begin{table}
\centering
{\scriptsize
\begin{tabular}{|c||c|c||c||c|c|}
\hline
\multicolumn{3}{|c|}{SpiderMonkey}&\multicolumn{3}{|c|}{GCC}\\
\hline
ID & Coverage & Repair & ID & Coverage & Repair\\
\hline
1 & 2,255 & - & 1 & 44,561 & -\\
2 & 720 & - & 2 & 75 & 1\\
3 & 190 & 1 & 3 & - & -\\
4 &  - & - & 4 & 3,067 & -\\
5 & 18 & 19 & 5 & 186 & - \\
6 & 46 & 28 & 6 & 490 & - \\
7 & - & - & 7 & - & -\\
8 & 6 & 3 & 8 &260 & 4\\
   &    &    & 9 & 465 & - \\
   &    &    & 10 & 19 & - \\
   &    &    & 11 & 43,903 & 10 \\
   &    &    & 12 & 85 & 1 \\
   &    &    & 13 & 2 & - \\
   &    &    & 14 & 302 & 34 \\
   &    &    & 15 & 223 & 5 \\
   &    &    & 16 & 639 & - \\
\hline
\end{tabular}
}
\caption{Coverage localization results}
%\vspace{-0.2in}
\label{covtable}
\end{table}


Table \ref{covtable} compares the {\bf Coverage} localization with the {\bf Repair} localization.  In general, obviously, {\bf Coverage} is not as effective as {\bf Repair}.  In fact, in cases where both methods provide any localization information, there is only one instance where {\bf Coverage} is better than {\bf Repair}.  On the other hand, {\bf Coverage} provides a localization for 10 faults where {\bf Repair} offers no assistance.  The rankings for these suggestions are often poor (indeed, in some GCC cases, they are almost ludicrously bad).  However, for GCC faults 10 and 13, {\bf Coverage} produced a useful localization when all of the other methods (including MUSE and MUSEUM) failed to localize at all.  Because compiler faults are so difficult to understand and fix, it might be useful for developers to examine the top few localizations (that differ) for multiple methods.  While MUSE and MUSEUM both generally perform better than {\bf Coverage}, they almost never provide better information than {\bf Repair}.  In every case where MUSE or MUSEUM provides a useful localization (within top-25) and performs better than {\bf Repair}, the difference in ranking between MUSE or MUSEUM and {\bf Repair} is five or fewer ranking positions.  {\bf Coverage} uses a sufficiently different approach that it offers potentially useful information, even when {\bf Repair} performs badly.

\input{othertools}

\subsection{Discussion}

In one sense, the discovery curve improvements here are practically very useful, but not extremely large in a relative sense.  The SpiderMonkey curve has an APFD only slightly more than 2\% better than the best result from previous FPF efforts.  For GCC, APFD improves by 6.4\%.  However, the comparison is with the very \emph{best} curve chosen after running more than 16 different metrics.  In practice, users simply do not know ground truth to rank curves, thus Chen et al. \cite{PLDI13} do not really give a practical approach to distance metric selection.  The best methods for different subjects varied widely, even in such difficult-to-understand ways as less-fine-grained coverage providing better results in some cases, but worse in other cases.   In practice, their work established that FPF could produce good curves, but gave very little useful guidance for choosing a metric for actual use of the technique, since trying multiple metrics is impractical: a user has to examine each curve.  The set of metrics also included methods such as comparing output signatures that are inapplicable to differential testing for wrong-code faults, the most critical compiler application.  This paper presents a single curve using a universal metric, and achieves a 2-6\% percentage point improvement over the best of more than 16 different metrics studied in the previous work; our improvements over any single ``reasonable'' method applied to both problems would be considerably larger (for instance, using the most obvious basis, line coverage, we see more than 17\% improvement).  Moreover, the improvement is, for our subject programs, reliable:  choosing random subsets of the full data set, the difference in mean APFD from best-previous method is around 3\%, and statistically significant.

For fault localization and error explanation, the results show possible improvement on state-of-the-art mutation-based approaches.  A practical impact of the results reported is that we suggest users of our techniques examine only the first few (at most 10, and we propose as few as 5) mutants and coverage-changing statements, and ignore localization if none of these results are helpful.  This is analogous to our suggestion that a user abandon the FPF curve if 5-10 test cases in a row fail to reveal any new faults, as a heuristic.  We suspect the information in the mutants/coverage changes is probably helpful in some cases where they do not localize a faulty line, but this is simply based on our highly incomplete understanding of the faults and patches in question, and not a solidly established claim.  The expertise of compiler developers would be needed to confirm or reject this belief.  Our suggestion that mutations themselves provide interesting error explanation is also applicable to MUSE and MUSEUM.  There may be some potential for confusing explanations, however, if repairs include mutants that also cause some failures, as in the standard MUSE/MUSEUM approach.  

For the compiler faults, if any repair for a failure modified a faulty statement, {\bf Repair} always ranked it in the top 34 localizations, ranked it in the top 5 in 6 of 10 such cases, and 3 times ranked it 1st.  MUSEUM, in contrast, ranked one such repair 195th, never ranked a fault in its top 3 statements, and only ranked a fault in its top 5 statements twice.  Of course, MUSEUM might be suffering from a lack of mutation operators, but in some cases this is unlikely due to the nature of the repairs.  For the CoREBench faults, {\bf Repair} produced more than twice as many perfect localizations as the other methods, though it did not provide any ranking for a fault in a large number of cases.

There are two ways to consider the performance of {\bf Repair}.  First, it is using information that other techniques are not using.  If each failure was only repaired by one mutant, MUSEUM and {\bf Repair} would both rank that mutant maximally, and we speculate that it would usually be faulty code.  But such cases are very rare in compilers:  for SpiderMonkey the mean/median number of mutants repairing each failure were 90.8 and 78, respectively, and for GCC the mean and median were 54 and 43.  {\bf Repair} can distinguish repairing mutants in fine-grained ways that MUSE and MUSEUM cannot by relying on the onion-ring structure:  when a repair also repairs failures that otherwise do not resemble the failure being localized (in terms of its repairing mutants), {\bf Repair} assumes that repair is general to many different faults, and so ranks it lower than more ``relevant'' repairs.  MUSEUM relies on a large set of mutation operators, and assumes that faulty statements will have multiple repairs, compared to non-faulty statements, or that faulty statements will be less likely to cause passing tests to fail when mutated.  Restricting ourselves to cases where {\bf Repair} and MUSEUM both provided a localization, we know that some faulty statement was in a repairing mutant, but that passing tests could not distinguish this mutant  from other repairs (since we only use repairs that cause no passing tests to fail).  Many of the faulty statement repairs (as expected for optimizing compilers) were statement deletions and conditional negations.  For these, it seems unlikely that Proteum would produce more mutants repairing the statement, though perhaps Proteum would create repairs for other statements that cannot be modified into a repair by any of Andrews' operators.  Our suspicion is somewhat confirmed by the results above where we used our methods with the MUSEUM mutant result set, and obtained similar results.

The other way to think about {\bf Repair} for the non-compiler faults especially, is as a very \emph{cautious} localization.  It only uses mutants that repair a failure, and, because it is clear that, in general, failing tests contain far more information about faults than passing tests, it only uses information from failures (other than using passing tests to prune mutants that kill some passing tests, again a principle of discarding potentially confusing information).  This results in {\bf Repair} either providing a ``useful'' localization or almost no localization information at all to mislead a user, for the non-compiler faults, which user studies and surveys suggest is the ideal behavior due to the cost of false positives in localization \cite{AutoHelp,Kochhar}.  This is basically a trade-off.  MUSE, MUSEUM, and Metallaxis-FL provide localization for many more faults, in a more diverse range of settings (with single failing tests, no repairs, etc.), and using more diverse information --- but, in cases where {\bf Repair} provides a localization, it seems to often be higher quality due to its restriction to a high-signal source of information (mutants repairing failing tests, only).  In settings, unlike compilers, where there are relatively few repairs, {\bf Repair} also produces very little incorrect information, and so has a low cost to the user in terms of wasted attention.  Note that our heuristic to stop reading after 5 wrong localizations is not even needed for all but one CoREBench fault --- {\bf Repair} suggests 1-3 statements at most.

Finally, in contrast to many previously reported results in fault localization (which are typically over much simpler programs and faults than subtle compiler semantics bugs) all methods we applied very frequently failed to provide a useful localization at all for our compiler faults.  This is presumably due to the sheer difficulty of compiler bugs:  our approach, and MUSEUM, performed very well on complex multi-lingual faults in the MUSEUM data set.  In one sense, {\bf Coverage} is the most intriguing method presented here.  While it did not perform nearly as well as {\bf Repair}, it did manage, in many cases, to rank \emph{some} faulty line highly, relative to the extremely large number of statements executed in each failing GCC run (on the order of 50K-100K+ statements).  In future work, we plan to experiment with different methods for weighting frequency of coverage, perhaps using methods from machine learning, or of pruning changed coverage that is not likely to be relevant.  For example, perhaps the {\bf Repair} approach can be applied, and coverage changes present in very distant failures can be removed, though the argument is less clear, since a coverage change isn't directly associated with the onion-ring structure, and does not directly repair a set of failures.  In fact, a study of the importance of delta-debugging in localization \cite{ReduceLocalize} showed that even traditional Spectrum-Based Fault Localization (SBFL) can \emph{sometimes} produce good results on SpiderMonkey faults.  Such methods, of course, often perform worse than even {\bf Coverage}; e.g., for our fault \#8 (their fault ID 115), {\bf Repair} and {\bf Coverage} produced localizations at rank 3 and 6, respectively, while the best SBFL result was well over rank 32.  We suspect that combining multiple methods, using some method to cross-assess confidence, might be required to obtain reliably good compiler bug localizations, and assign good confidence scores to localizations.  There is much to be done if automated debugging is ever to make the lives of compiler developers easier in most cases \cite{AutoHelp}, and a human study using real, complex compiler bugs would be ideal, since previous studies of localization effectiveness tend to focus on simple tasks even moderately experienced developers can potentially succeed at.  Fixing deep optimizing compiler bugs is a very specialized skill, in contrast to general debugging skills.


\section{Mutation Analysis Costs and Potential Optimizations}
\label{sec:highcost}

Our compiler experiments required a very large computational budget.  However, our approach is, by design, cheaper than MUSE or Metallaxis-FL, in that, for a program with up-to-date mutation testing results (just knowing which mutants are killed, not a full matrix), it only requires executing mutants over failing tests.  Running each test case under each mutated version of the compiler is cheap: each execution requires around 0.05 seconds for SpiderMonkey, and 0.12 seconds for GCC, on average; moreover, executions can be done in parallel, most failures do not cover most mutants, and vice versa, so the total number of repair checks needed is far less than the product of mutants and failures, and can be spread over many machines.  

Mutation cost reduction techniques are also applicable.  E.g., trivial compiler equivalence (TCE) \cite{TCE} can reject equivalent and redundant mutants, removing almost 30\% of mutants for benchmark subjects. Other techniques, such as using mutant schemas to avoid having to compile and store each mutant, are also applicable --- almost all of the techniques characterized by Offutt and Untch \cite{offutt2001mutation} as \emph{do faster} approaches apply, without any loss of ability to identify or localize faults.

\subsection{Hot and Cold Running Mutants}

Note that the largest cost in our approach, by far, is compiling the mutants.  While running all the GCC tests on the covered, compilable, test-passing mutants takes nearly 24 days of total execution time, the process is embarrasingly parallel:  no mutant/test execution depends on any other.  On the other hand, all executions of a mutant/test pair depend on having a compiler that includes that mutant built, and compiling GCC requires more time than test executions by a large margin.

We expect that fuzzer taming will mostly be performed when someone is fuzzing a compiler either for the first time or using a new technique.  If fuzzing is performed on a well-fuzzed compiler, with only incremental changes from previous versions, there is not likely to be a significant fuzzer taming problem:  fuzzing will often only detect a small number of faults, and manual triage efforts will probably suffice.  In the case of ``new'' fuzzing efforts, however, they will usually be aimed at release versions of a compiler.  For major projects such as GCC, LLVM, or V8, producing and publishing a set of ``hot'' mutants might be one solution.  \emph{Hot} mutants are mutants that 1) compile 2) pass all current tests and 3) are not known to be equivalent mutants.  Producing such mutants should be an easily automatable task without prohibitive cost, at the level of a major open source project.  Release versions are only produced a few times a year (e.g., both GCC and LLVM had 5 releases in 2018 and 4 in 2017).  In addition to fuzzer taming, such mutants would provide a useful quick benchmark for new compiler-fuzzing techniques:  if a technique can kill mutants not detectable by the compiler's test suite, it has possibilities for bug detection as well.

Alternatively, a radical way to produce ``hot'' mutants at low cost (the only significant cost being that of executing the test suite to throw out mutants killed by some passing test) is to produce mutants at the \emph{object code} level, which does not require recompiling a system.  This is how SQLite applies mutation testing:  \url{https://www.sqlite.org/testing.html#mutation_testing}.  Object level mutants are obviously not useful (or at least, certainly not \emph{as} useful) for fault localization as source-level mutants, but mutating branches alone probably captures many compiler repairs that are useful in fuzzer taming.

\subsection{Lossy Techniques}

\begin{figure}
  \centering
  \includegraphics[width=0.95\columnwidth]{samplejs}
  \caption{APFD values for Spidermonkey samples}
  \label{samplejs}
\end{figure}%

\begin{figure}
  \centering
  \includegraphics[width=0.95\columnwidth]{samplegcc}
  \caption{APFD values for GCC samples}
  \label{samplegcc}
\end{figure}%

\subsubsection{Random Sampling}

Mutation cost reduction techniques classified as \emph{do fewer} on the other hand involve a loss of signal, not just more efficient production of the same signal.   One of the most popular and simple approaches to reducing the cost of mutation testing is \emph{random sampling} \cite{budd1980mutation,acree1980mutation,RahulISSRE,MutRand}.
 Figures \ref{samplejs} and \ref{samplegcc} show, respectively, the results of random sampling of mutants for SpiderMonkey and GCC, by comparing the APFD obtained on a set of 30 random samples of the chosen percentage of mutants with the APFD for the full mutant set.  Because some mutants are actually misleading with respect to distinguishing faults, sometimes sampling can improve results.  Sampling 10\% of mutants directly translates to our approach requiring about 90\% less time (or less computing power if run in parallel).  We assume a ``hot'' mutants approach in that our sampling is of compilable, test-passing mutants.  A larger sample would be required, in proportion to how many mutants compile and pass tests, to achieve the same results in a scenario with only ``cold'' mutants, though the difference is not huge, since most mutants are ruled out by the fact that no failing test covers the mutant, rather than that they cause some passing test to fail.  The more significant difference in a cold-mutant scenario is that each mutant/test evaluation is of course much cheaper without the cost (even amortized over many tests) of compilation, not the total number of mutants considered.

Pure random sampling is fairly effective for GCC, but more mutants are required to obtain a comparable APFD on average, for SpiderMonkey.  The reason for this difference is not easy to understand.  At a high level, every mutant that repairs at least one failing test test case provides at most one bit of information for fuzzer taming.  Some repairing mutants repair all test cases (thus providing no information to distinguish faults), and others provide potentially redundant information in that they repair the same set of mutants as another mutant.  Mutants that repair no test cases provide no information, of course.  Given that more SpiderMonkey mutants (12.6\%) repaired a test case than did GCC mutants (7.8\%), it would seem that a \emph{smaller} sample of mutants would be required to produce the ``same number of bits of information'' with SpiderMonkey.  However, SpiderMonkey mutants are more redundant than GCC mutants, and, perhaps more importantly, the SpiderMonkey code is less well-structured, and changes behavior for more causally complex (and unrelated to faults and clear optimization levels) reasons than the more mature GCC code, making each mutant's bit of information inherently more ``noisy.''
For GCC, using only 30\% of the mutants produces results quite close to the optimal results, and almost never drops below 90\% effectiveness.  For SpiderMonkey, similar results come with 60\% of mutants.  Sampling is an inherently incremental process:  mutants can be processed in a random order, and a developer can compute the curve at any point in time.  We observed that the early curve is reliably good for both SpiderMonkey and GCC, even with as few as 10\% of mutants; most variance is in the further reaches of the curve.  If developers can identify and fix bugs based on early mutant results, these can be fed back into the process, and mutants not covered by any remaining unfixed tests can be removed from the set.  For fault localization, with hot mutants a single failing test's complete repair set can be found with relatively few evaluations, and then the repairing mutants can be evaluated on all other tests, to compute a localization that is essentially as accurate as the best ones we produced (distances may be slightly off due to mutants repairing other tests, but not the test being localized, but the sampling is likely to largely handle this issue, since exact distances are not important, only relative ones).

We tried various ways of improving the effectiveness of random sampling by forcing the inclusion of certain mutants that were deemed more likely to be useful.  For example, it is very important to include mutants that repair hard-to-repair faults represented by a small number of tests.  Finding faults represented by few tests is, in some sense, what users want from a fuzzer taming algorithm.  We cannot, without evaluating them over tests, predict which mutants will repair only a small number of tests (and thus, likely, faults with only a few representatives).  However, making sure to include mutants that are only \emph{covered} by a small number of tests lets us ensure we cover mutants that, if they repair any failing tests, only repair a small number of failing tests.  However, due to the likelyhood that such mutants repair no tests at all, including mutants covering at most $k$ tests (for small $k$) turned out to be either actually harmful (statistically significantly worse APFD) or statistically indistinguishable from pure random selection.  We were unable to find any mixes of random and biased or chosen sampling that performed better than pure random sampling.  Is this the best we can do?


\subsubsection{Operator-Based Selection}

A deterministic alternative to random sampling, long used in the mutation testing community, is operator-based selection, where only some mutation operators are applied \cite{wong1995reducing,untch2009onreduced}.  Using \emph{only statement deletion and condition negation mutants}, as in our toy example, on the grounds that we expect most compiler ``repairs'' to simply avoid an optimization, not actually fix a simple defect such as an off-by-one error in a loop, is an intuitively appealing and trivial to implement approach.  The idea is clearly related to the object-level branch-mutation proposed above, conceptually, but operating at the (more understandable to humans, and thus useful in localization) source level.  The resulting cost reduction is significant:  67.8\% of mutants can be ignored for SpiderMonkey, and 73\% for GCC.

More importantly, the APFD actually \emph{improves} with this change.  For SpiderMonkey, the APFD improves by almost 3\%, and for GCC it improves by 0.86\%.  For SpiderMonkey, the early curve (first ten items) is improved, and for GCC it is very slightly worse.  Most importantly, in both cases, \emph{two additional faults} are revealed in the first 50 examined tests.  We speculate that the accidental nature of many repairs produced by operator and constant replacements may introduce harmful noise into the distance metric, and thus the FPF-based fuzzer taming process.  Changes that simply sidestep faulty functionality are, perhaps, more likely to be meaningful \cite{achour}.

Operator selection can, in principle (and in one case in our data set, in practice) reduce the effectiveness of fault localization.  However, with hot mutants, it is easy, if localization using only negations and deletions is not helpful, to use the same method as proposed above for random selection to compute a more precise localization for a test.  This should usually not be required, however:  we noted that in most cases where a mutant involved in a localization was an operator replacement, the mutant was usually equivalent in behavior to a condition negation.  Constant replacement repairs were simply very rare and not involved in any of the useful compiler localizations.  The problem observed with random sampling, that the metric of distances to other tests may be inaccurate, is of much less concern with operator selection:  as far as we can tell, based on FPF discovery curves and APFD, distances are \emph{more} accurate using only the mutants we propose.

While using only some mutation operators \emph{may} degrade fault localization performance for non-compiler faults, we propose it as the default way to use our approaches in compiler fuzzer taming  and debugging scenarios.  Furthermore, we speculate, based on the results of Qi et al. \cite{achour} showing that most actual correct fixes in program repair are functionality deletions, that limiting mutants used to condition negations and statement deletions (or even adding an operator that converts conditionals to a {\tt False} constant, as is provided by some tools \cite{RegExpMut}), might be the best practice for complex programs and ``deep'' (hard to debug) faults in general.  It seems unlikely that mutation operators will produce many fixes that do not simply avoid broken functionality (vs. actually fix it), for the kinds of faults that are actually hard for programmers to locate, understand, and correct.

\subsubsection{Combinatorial Control of Optimizations}

Finally, if we assume that in the setting of compilers, faults are due to optimizations, it may be possible to use a combinatorial approach to turning on/off optimizations as a cheap equivalent of mutants; Ghandehari et. al \cite{CombTestFaultLoc} proposed a combinatorial approach to fault localization and such a method might work fairly well in this setting.  Further experimental effort would be required to determine how useful such a method would be; a major concern is that rather than simply ``turning off'' optimizations as one might at the command line, some mutants change the result of a check as to whether an optimization is valid for a piece of code.  GCC and LLVM also do not, unfortunately\footnote{Unfortunately, at least, from this combinatorial point of view; as developers we may suspect that modern compilers already have plenty of command-line options.}, expose all the optimizations they perform as individually controllable by users.  Based on our examination, the level of control over optimizations provided by GCC 4.3.0 was simply too coarse-grained to enable effective replacement of mutants, and SpiderMonkey 1.6 offers almost no control over optimizations, as would be expected of a JIT usually used in a browser context; it does not work as a command-line tool that takes an input file and produces a binary.  There may be API calls to control optimization in some way, but these were not easy to discover, or practical for a developer to use.

\section{Fuzzer Taming for A Non-Compiler Target: Fuzzgoat}

%\subsection{Fuzzgoat}

\begin{figure}
   \centering
        \includegraphics[width=1.0\textwidth]{afl_fuzzgoat.png}
        \caption{AFL run for fuzzgoat}
        \label{aflfuzzgoat}  
  \end{figure}

  Fuzzgoat (\url{https://github.com/fuzzstati0n/fuzzgoat}) is a well-known backdoored C program used to test fuzzers and other analysis tools.  It is an interesting subject to consider non-compiler fuzzer taming because it is a single program version with 4 distinct bugs with complete ground truth, and widely viewed as representative of a typical fuzzing use-case.  We ran AFL \cite{aflfuzz} for over 7 hours on the program, generating 4,244 crashing inputs (Figure \ref{aflfuzzgoat} shows a screenshot from just before we terminated the run).  AFL's internal triaging methods reduced this to a set of 38 crashes it considered distinct.  We terminated the run once it had been nearly three hours since a new distinct crash was detected.

  These crashes exhibit a classic fuzzer taming structure.  Three of the bugs are easy to identify: they are triggered by 15, 14, and 11 of the 38 AFL-produced crashing inputs, respectively, and all have multiple inputs that uniquely detect that bug, and no other.  The fourth bug, however, is only triggered by \emph{three} of the 38 inputs, and is never uniquely present (any input triggering it also triggers at least one other bug).  The bug frees a pointer that can subsequently be used, and it is (as often with use-after-frees \cite{DangSan}) hard to produce inputs that actually cause this to happen. This bug was only found by AFL after nearly an hour of fuzzing, in fact.

  This is a classic fuzzer taming problem, albeit on a smaller scale than with the compilers:  a user will probably not want to inspect the highly-redundant set of 38 inputs, but will want to see all four distinct faults, while examining as few inputs as possible.

  Applying the mutation-repair based approach is relatively easy here.  We used the universalmutator tool \cite{RegExpMut} (\url{ https://github.com/agroce/universalmutator}) to generate 18,489 mutants for fuzzgoat.  This tool is far more aggressive than Andrews' tool, in terms of operators used, and, more importantly is more widely available to typical users and easy to apply to source code in a large set of languages, unlike, e.g., Proteum.  Of these mutants, only 6,809 actually compile, and trivial compiler equivalance \cite{TCE} can further reduce the set to 2,899 mutants to be used in producing repair bitvectors.  Computing the full bitvector set and all distances for the 38 crashes required just over 10 minutes.  While compiling and running mutants is a serious computational burden in the setting of a production compiler, it is negligible compared to the time required for fuzzing, in a typical fuzzing effort, we suspect, even without using such simple additional reductions as checking the mutants for code covered by some crashing input.  This is partly because the number of mutants to consider is smaller, and partly because compiling GCC or SpiderMonkey is simply much slower than compiling more typical small-to-medium-sized fuzzing targets.  Fuzzing a very complex target, such as a complex production-grade database (e.g., LevelDB or RocksDB) or the Linux kernel itself might be more like compiler fuzzing, however, in terms of mutation analysis cost.  Input runtime is also smaller for most non-compiler targets, we suspect, even including ``hard'' targets like databases and kernels.
  
\begin{table}
\centering
{\scriptsize
\begin{tabular}{|l||r|r|r|r|}
\hline
Method & Cutoff & Mean bugs & \# Perfect & \% Perfect\\
  \hline
  Repair & 3 & 3.00 & 0 & 0.00\% \\
  Coverage & 3 &  3.10 & 13 & 34.21\% \\
  Random & 3 & 2.58 & 3 & 7.89\%\\
  \hline
  Repair & 4 & 3.42 & 16 & 42.10\%\\
  Coverage & 4 &  3.39 & 15 & 39.47\% \\
  Random & 4 & 2.74 & 6 & 15.79\%\\  
  \hline 
    Repair & 5 & 4.00 & 38 & 100.00\%\\
  Coverage & 5 &  3.50 & 19 & 50.00\%\\
  Random & 5 & 3.29 & 16 & 42.10\%\\  
  \hline
    Repair & 6 & 4.00 & 38 & 100.00\% \\
  Coverage & 6 &  3.50 & 19 & 50.00\%\\
  Random & 6 & 3.18 & 11 & 28.95\%\\
  \hline
  \multicolumn{5}{c}{$\ldots$} \\
\hline 
    Repair & 10 & 4.00 & 38 & 100.00\% \\
  Coverage & 10 &  3.63 & 24 & 63.16\%\\
  Random & 10 & 3.58 & 23 & 60.53\%\\  
  \hline
    Repair & 11 & 4.00 & 38 & 100.00\% \\
  Coverage & 11 &  3.63 & 24 & 63.16\%\\
  Random & 11 & 3.66 & 25 & 65.79\%\\  
  \hline
    \multicolumn{5}{c}{$\ldots$} \\
  \hline
    Repair & 17 & 4.00 & 38 & 100.00\% \\
  Coverage & 17 &  3.97 & 37 & 97.37\%\\
  Random & 17 & 3.84 & 32 & 84.21\%\\  
  \hline
    Repair & 18 & 4.00 & 38 & 100.00\% \\
  Coverage & 18 &  4.00 & 38 & 100.00\% \\ 
  Random & 18 & 3.82 & 31 & 81.58\%\\  
  \hline
\end{tabular}
}
\caption{Fuzzgoat triage results}
%\vspace{-0.2in}
\label{table:fuzzgoat}
\end{table}

Table \ref{table:fuzzgoat} shows fuzzer taming results for fuzzgoat, using our repairing-mutant-based approach, Chen et. al's \cite{PLDI13} statement coverage method, and (as a baseline) random ordering.  For repair-based ranking and statement coverage-based ranking, we show the mean result over all 38 possible choices for first test to rank.  Random is the mean over 38 random rankings.  {\bf Cutoff} indicates the point at which the user stops examining crashing inputs, and {\bf Mean bugs} indicates how many of the four distinct faults, on average, the method, with that cutoff, allowed the user to see within that cutoff.  {\bf \# Perfect} describes how many of the 38 experiments produced a perfect triage, where all of the bugs were seen before the cutoff.  We begin {\bf Cutoff} at 3, since 3 is the minimum possible number of inputs a user must examine to find all four bugs (some inputs cause a crash due to either of two bugs, so are able to identify two faults).  Coverage-based FPF is better than repair-based FPF, for fuzzer taming, if a user only examines 3 of the 38 crashes.  However, once a user is willing to look at 4 inputs, mutation-repair is better, and if the user is willing to look at 5 of the 38 inputs, our approach guarantees all of the distinct bugs will be seen.  Statement coverage-based FPF, in contrast, requires a user to look at {\bf 18} inputs before providing such a guarantee.  Indeed, there is a more than 25\% chance of missing one bug even if the user looks at 10 inputs, and random ranking performs almost as well as coverage-based FPF, once a user is willing to examine 10 or more inputs.  In the table, cutoffs for which statement-based coverage performance is identical to the previous cutoff are omitted.  That the coverage-based ranking does not improve its performance between cutoff of 6 and 9, and 11 and 16, shows how weak the signal from the distinct faults is, in terms of code coverage.

To our knowledge, the state-of-the-art in research on advancing fuzzing triage over the tools built into fuzzers like AFL is represented by the \emph{semantic crash bucketing} (SCB) approach of van Tonder, Kotheimer, and Le Goues \cite{SCB}.  This approach uses patch templates based on common types of bugs/semi-fixes, combined with semantic information obtained by observing the execution of a crashing input, to bucket crashing inputs discovered by fuzzers.  We contacted the SCB authors, and with their cooperation investigated how well SCB handled the fuzzgoat crashes.  The fuzzgoat code highlights the primary limitation of SCB: namely, if a kind of failure is not handled by a (manually constructed) patch template and identified by the semantic feedback mechanisms, then SCB does not really handle it.  The SCB tool was able to produce a patch for one of the four fuzzgoat bugs, a null pointer dereference.  Unfortunately, at present, SCB does not handle bugs related to misuse of {\tt free} (use-after-free, double frees, or frees of invalid pointers), and the other three fuzzgoat bugs are all {\tt free}-related.  SCB therefore can very effectively identify the inputs associated with one of the four bugs (when it is the only one triggeered by the input), but provides no assistance with finding the other three bugs. This is not to say that SCB is useless here; the null pointer bug alone is responsible for 13 of the 38 crashes, so SCB does make it easy to see one example of the null pointer bug, and ignore another 12 inputs.  It does not, however, help find the rare bug with only a few instances among the remaining 25 inputs.  While mutation repair also has no knowledge of {\tt free} bugs, it is able to find mutants that avoid their symptoms or their causes, and thus triage all four faults effectively.   The SCB team added the fuzzgoat example to their published VM, and plan to use it to drive development of patch templates and analysis for use-after-frees; this will, at some point, likely make it possible for SCB to handle fuzzgoat effectively.  However, in the long term, if fuzzing is applied to find assertion violations, differential testing divergences \cite{Differential,ICSEDiff}, or other more complex ``custom'' properties, as is likely with the appearance of tools such as DeepState \cite{DeepState} that merge fuzzing and property-based testing, it is hard to see how SCB will handle these ``custom'' bug types.

Fuzzgoat is a poor candidate for fault localization, due to the lack of a reasonable set of passing tests, and due to the fact that all four bugs are fairly easy to understand once an appropriate input is run under a sanitizer.  We do note that repair manages to localize the null pointer dereference perfectly (SCB's patch is also an excellent localization for this bug).  Given passing tests, we suspect all fault localization methods discussed in this paper could handle the localization problem easily, in contrast to a typical compiler bug.

\section{Threats to Validity}

The largest threat to validity is that this paper's core conclusions rely on only two data sets for compilers, and a small number of non-compiler faults.  The compiler ground truths are possibly imperfect, due to the complex change history, and it was not possible to produce good patches for all faults, limiting our analysis of compiler fault localization.    The comparison with other fault localization methods is also limited:  our compiler experiments limit the mutation budget in ways that may weaken MUSE and MUSEUM effectiveness, and the results using fault and mutant data sets from other studies provide few faults, no multi-fault problems, and very few multi-failure problems, limiting {\bf Repair} effectiveness.   It is also possible that faults in the framework used for experiments introduced errors into our results.  In order to make it possible to check for these problems, we have made the raw data for a re-analysis available: \url{https://github.com/agroce/mutants16/tree/master/tests}.  The data required for analysis of the non-compiler faults is available from the authors of the respective studies (see the acknowledgements at the end of the paper).
