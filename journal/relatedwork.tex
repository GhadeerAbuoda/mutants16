\section{Related Work}
\label{sec:related}

There are four primary areas of related work.  First, fuzzer taming and related problems of determining the set of faults from a large set of redundant failures have been investigated in a few papers.  Second, some recent efforts to localize faults have begun to incorporate the use of program mutants, and the broader problem of fault localization has been very extensively investigated.  Program mutants have also been used in efforts to produce automatic repairs for program faults.  Finally, our work fits into the general framework of efforts to define distances between program executions.
 
The most closely related work (other than the ISSRE 2018 \cite{ISSRE18Mutants} paper that we extend) is that of Chen et. al \cite{PLDI13}, whose FPF \cite{Gonzalez} algorithm and benchmark we use.  We extend their work with a better, and more universal distance metric, as well as an approach to fault localization in addition to fuzzer taming.  Prior to the FPF-based work, Francis et al. \cite{Podgurski04}, Podgurski et al. \cite{Podgurski03} and others \cite{Liu06,Liblit05} used clustering to attempt to solve similar problems in identifying distinct faults, on simpler human-generated data sets.  Chen et. al \cite{PLDI13} provide an in-depth comparison of clustering and ranking, and the advantages of ranking (as well as some possible limitations).  Jones et al. discuss general issues in debugging multiple faults \cite{Jones07}.  The work of Groce et al. \cite{OneTest} provides an alternative approach to fault identification, based on term rewriting to normalize failures, but is not currently applicable to compiler fuzzing, as it only supports tests that are sequences of method calls in Python \cite{ISSTA15,tstlsttt}.

One recent approach does, at a conceptual level, resemble our methods:  \emph{semantic crash bucketing} (SCB) improves the fuzzer taming ability of AFL \cite{aflfuzz}, Honggfuzz \cite{honggfuzz}, and CERT BFF \cite{BFF}, by approximating real bug fixes with lightweight program modifications \cite{SCB}.  The primary difference between SCB and our approach is that we generate many more potential approximate fixes, via mutation, but use no semantic feedback from failing inputs other than code coverage.  The selectivity of SCB dramatically improves its scalability and ease-of-use, but limits applicability to cases where a patch template has been manually constructed, e.g., buffer overruns and null pointer dereferences.  While these cases are by far the most common in traditional security/crash-based fuzzing, crashes are usually the \emph{uninteresting} cases in compiler fuzzing, or fuzzing/symbolic-execution based property-based \cite{ClaessenH00} testing \cite{DeepState,DeepStateTutorial,deepstaterepo}.  It is unlikely that enough patch templates for the interesting changes to compiler code, or invariant-preserving code in other fuzzing, can be manually produced to make SCB alone generalize to these settings.  Another difference is that SCB provides bucketing, rather than ranking; this is in some cases ideal (a user of AFL usually just wants to count ``different'' vulnerabilities that are memory-safety based), but in settings where the fixes are even more approximate and overlapping, and the bugs more complex, ranking is most useful.  

Fault localization \cite{FaultSurvey,NearNeighbor,Liblit03,Liblit05,Cleve05,Jones2002,Jones05,Jones07,GroceError,ChakiLev,Liu06,SPIN03,Santelices:ICSE:2009,Abreu:2006:PRDC,xuan2014test,DD}, even specifically for compilers \cite{Whalley94}, is a long-standing field of research.  Recently, fault localization researchers have used program mutants to improve statistical \cite{Jones2002} fault localization techniques \cite{MUSE,DebroyMutant,FasterMutant,multilingual,Metallaxis,Papadakis}.
The core of this work has been the belief that faulty program locations are (1) more likely to change a failing test to successful when mutated and (2) less likely to change a successful test case to faulty when mutated.  This clearly connects to our general notion that how two test cases respond to a set of mutants is a good measure of their semantic similarity.
The work of Hong et al. \cite{multilingual} on MUSEUM and Moon et al. \cite{MUSE} on MUSE provides a good summary of other work along these lines, and is perhaps most similar to ours in assumptions. In particular we agree with ``Conjecture 1'' that a test case that used to fail on a program is more likely to pass on a mutation of a faulty statement.  However, while we suggest some localizations as a welcome and interesting side effect of our distance metric and FPF analysis, our primary goal is the discovery of all faults. Much fault localization work is largely (in many cases exclusively) based on single-fault scenarios; e.g., the MUSE \cite{MUSE} evaluation includes only 2 of 14 scenarios with multiple faults, and those scenarios include only a few faults.   The statistical approaches may owe some of their superiority in experiments over other methods to the use of mostly single-fault evaluations \cite{Jones05}.  By focusing on a single failing test case (for us, selected by fuzzer taming) as the locus of analysis, our work, like some early efforts \cite{NearNeighbor,Cleve05,GroceError} may be more suited for situations involving many faults.  MUSEUM \cite{multilingual} also revisits this approach, combining the MUSE formula with localization using only a single failure.

Explaining, rather than merely localizing, bugs is a less common goal, though all mutation-based approaches can probably be adapted to this idea (since a mutation is, itself, a kind of explanation).  Key work on explaining bugs in addition to localizing them includes Groce's early model-checking work \cite{GroceError,ChakiLev} and the MintHint \cite{MintHint} approach, which bears some resemblance to the use of mutants as ``hints'' for fault repair, but is more general and backed by user studies to show its utility.  The real-world value of MintHint explanations supports the idea that mutant-based explanations might be useful to developers.

Using program modifications or mutants broadly defined to repair programs has been a popular topic of recent work as well \cite{GenProg,AutoRep,WeiFix}.  These \emph{generate-and-validate} approaches to patching aim to fix programs by generating a large variety of potential fixes, then using test cases to prune out invalid fixes.  Qi et al. \cite{achour} discuss some serious limitations of early work on this topic, but also demonstrate that effective patching is possible using a restricted search space that focuses on removing functionality.

Some approaches to localization or patching assume the possibility of a mutant/change \emph{actually} fixing a fault.  In our experiments, we found this to be highly unlikely -- most of our compiler patches were far larger than the largest higher-order-mutants \cite{jia2009higher} anyone usually considers applying, and \emph{none} were equivalent to a repair.  Previous work has shown that, across a large body of open source programs in many languages, most fault fixes were larger in size than even low-order higher-order mutants \cite{GopinathMutants}.
We speculate that fixes for compilers may be likely to be even more complex than the average, given that compilers are particularly complex software systems.  
Our primary assumption is simply that similar executions respond similarly to mutants, and in particular executions due to the same fault can be ``repaired'' by similar program changes.  These changes are likely to be \emph{related} in some causal way to the actual fault, but are unlikely to be the actual fault, for reasons of comparative complexity of mutants and real fixes.

The use of distance metrics in software engineering includes some of the localization and fault identification efforts discussed above (e.g., \cite{NearNeighbor,GroceError,ChakiLev,Liu06}).  Vangala et al. proposed using distance to cluster test cases to improve diversity and eliminate duplicates \cite{VangalaDist}.  We speculate that using FPF with our causal metric over passing tests as a test prioritization (or selection) method might be a fruitful approach \cite{YooHarman}.  Adaptive random testing uses distances (usually between inputs) to choose tests \cite{Chen,ARTChen,ISSTAART}, and Artzi et al. guided test generation for fault localization using path constraint metrics \cite{ArtziDirected}.  Sumner et al. evaluated approaches to generating execution peers for debugging and understanding \cite{Sumner2011} using tree edit distances (and evaluated other distances).  Xin et al. \cite{Xin2008} examined methods for indexing program executions, useful in all methods involving execution alignments.  To our knowledge, most metrics used are essentially spectrum-based \cite{RepsSpectra,BallConcept} (using counts over structural entities), while our metric is essentially causal, based on mutants as counterfactual \cite{LewisCause,LewisCount} versions of a program.