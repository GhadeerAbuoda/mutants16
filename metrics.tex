\section{A Mutant-Based Distance Metric}

For every failing test case, its response to mutants defines a bit-vector, with
length equal to the number of mutants that repaired any test case,
where a 1 bit indicates that the mutant in question repairs this test
case.  These cannot be effectively compared with Hamming distance. Consider $t_1$ and $t_2$, both repaired
by 100 mutants.  If these two test cases share 90 common repairing
mutants, and disagree for 10, they have a Hamming
distance of 20.  This makes them ``less similar'' than two test cases,
$t_1'$ and $t_2'$, where $t_1'$ is repaired by 5 mutants, $t_2'$ is
repaired by 5 mutants, and \emph{none of those mutants overlap.}   Hamming distance for the first pair of tests is twice as
large, despite the fact we have no evidence $t_1'$ and
$t_2'$ are due to the same fault, in contrast to strong evidence for $t_1$ and $t_2$.  A Jaccard distance, used in some fault
localization algorithms \cite{Liu06}, is a better fit.  For bitvectors $u$ and $v$ of length $n$:
%\vspace{-0.08in}
\[d(u,v) = 
\begin{cases}
\frac{\sum_{i=0}^{n-1} 1\ \text{if}\ u_i \neq v_i\ \text{else}\ 0}{\sum_{i=0}^{n-1} 1
\  \text{if}\ u_i \neq 0 \vee v_i \neq 0\ \text{else}\ 0} & \exists i:u_i\neq 0 \vee v_i\neq 0\\
0 & otherwise
\end{cases}
\]

This metric, the portion of mismatches over bits that are 1 in either
vector, matches our intuitions about the above comparisons.  It
essentially formalizes the intuition of the onion-ring model, where
more matching repairs indicates a much higher probability two failures
are due to the same fault, even if not all repairs match.

Given such a metric, the FPF \cite{Gonzalez} approach to fuzzer taming \cite{PLDI13}
begins by selecting an arbitrary test case.   FPF then proceeds by selecting the test
case, of all unranked tests, that has the largest minimum distance
from all already ranked test cases, repeating this until all tests are
ranked.  In other words, FPF always chooses a test case that is as dissimilar as
possible from all previously chosen test cases, based on the \emph{closest
already chosen} test case.  FPF is boundedly close
to optimal under certain assumptions \cite{Gonzalez}.

More complex metrics are possible, also combining information such as
coverage and language features or test case tokens \cite{PLDI13}.
However, mixtures of metrics (combining different features) other than
Levenshteins \cite{lev} over test case + output never performed well
compared to more focused metrics in earlier work \cite{PLDI13}.
Therefore, in order to evaluate the contribution of our repair-based
metric we compare it, alone, to the previous best metrics.

In Section \ref{conc} we briefly discuss possible uses of $d$
requiring comparison of both failing and passing test cases. The only
change required is to use three-valued vectors, where 1 indicates a failing property succeeds and -1 indicates a succeeding property fails.
